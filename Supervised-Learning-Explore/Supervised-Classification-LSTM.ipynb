{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised-Classification-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4hREtqH5E7"
      },
      "source": [
        "***Warning: DO run this notebook on Google Colab (Pro preferred) instead of local environments.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n22j2irpFh4x"
      },
      "source": [
        "# Import packages.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D3AgRojFh42",
        "outputId": "7eb52653-dc60-4ee1-da09-d919cf7d5ccd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adG59Sv3KyIX"
      },
      "source": [
        "root_dir = 'gdrive/Shareddrives/MADS-Capstone-haizhou/assets/'\n",
        "df_train = pd.read_csv(root_dir + 'df_train.csv')\n",
        "df_dev = pd.read_csv(root_dir + 'df_dev.csv')\n",
        "df_test = pd.read_csv(root_dir + 'df_test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oehE7MCKHmvA"
      },
      "source": [
        "# Preprocess Strings (Lemmatize, Remove Stopwords, etc.)\n",
        "df_train['tweet_text'] = df_train['tweet_text'].apply(lambda x:\" \".join(preprocess_string(x)))\n",
        "df_dev['tweet_text'] = df_dev['tweet_text'].apply(lambda x:\" \".join(preprocess_string(x)))\n",
        "df_test['tweet_text'] = df_test['tweet_text'].apply(lambda x:\" \".join(preprocess_string(x)))\n",
        "\n",
        "# map labels to ordinal values\n",
        "unique_labels = df_train['class_label'].unique()\n",
        "dict_label = dict(zip(unique_labels,np.arange(len(unique_labels))))\n",
        "y_train = df_train['class_label'].map(dict_label)\n",
        "y_dev = df_dev['class_label'].map(dict_label)\n",
        "y_test = df_test['class_label'].map(dict_label)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97HZPOYBFh43"
      },
      "source": [
        "# clean the texts for further tokenization.\n",
        "df_train['tweet_text'] = ' ' + df_train['tweet_text'].astype(str)\n",
        "df_dev['tweet_text'] = ' ' + df_dev['tweet_text'].astype(str)\n",
        "df_test['tweet_text'] = ' ' + df_test['tweet_text'].astype(str)\n",
        "X_train = df_train.tweet_text\n",
        "X_dev = df_dev.tweet_text\n",
        "X_test = df_test.tweet_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZZlssKhFh43"
      },
      "source": [
        "# Tokenize documents based on dict-like mapping.\n",
        "vocab_size = 5000\n",
        "tokenizer = Tokenizer(num_words = vocab_size,oov_token = '<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Use the trained tokenizer to convert the documents to sequences.\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "dev_sequences = tokenizer.texts_to_sequences(X_dev)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utJkceLGFh44",
        "outputId": "c48bdaf5-74b9-4aa0-bd95-f1d64f1bbd9b"
      },
      "source": [
        "length_train = [len(sequence) for sequence in train_sequences]\n",
        "length_dev = [len(sequence) for sequence in dev_sequences]\n",
        "length_test = [len(sequence) for sequence in test_sequences]\n",
        "max_length = max(length_train+length_dev+length_test)\n",
        "max_length"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0_JN6lFFh45"
      },
      "source": [
        "# Padding: Add zeros to the end of the sequences, to ensure same length accross sequences.\n",
        "train_padded = pad_sequences(train_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "dev_padded = pad_sequences(dev_sequences, maxlen = max_length, padding = 'post', truncating = 'post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen = max_length, padding = 'post', truncating='post')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAK457e3Fh45"
      },
      "source": [
        "embedding_dim = 100\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size,embedding_dim),   # Embedding layer\n",
        "    tf.keras.layers.LSTM(embedding_dim),   # LSTM layer\n",
        "    tf.keras.layers.Dense(10,activation = 'softmax')   # Final fully-connected layer\n",
        "])\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRfKTloepmdp",
        "outputId": "76dcbe2d-d1dd-4e59-d93d-3342a81a732c"
      },
      "source": [
        "# What is the model's initial performance on the dev set?\n",
        "model.evaluate(dev_padded,y_dev)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237/237 [==============================] - 6s 22ms/step - loss: 2.3117 - accuracy: 0.0537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3101727962493896, 0.071390800178051]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsOI74FhqM_v",
        "outputId": "a0737c1a-41c5-45d2-8bbc-aeee53719916"
      },
      "source": [
        "# I have saved the trained model into the shared drive.\n",
        "# model.load_weights(root_dir+'BiLSTM-checkpoints/checkpoint.ckpt')\n",
        "\n",
        "# with open(root_dir+'BiLSTM-checkpoints/history.pickle','rb') as handle:\n",
        "#   history = pickle.load(handle)\n",
        "\n",
        "# Here's how I fitted the model in the first place.\n",
        "# Although in saving the history, I also added the epoch numbers and epoch-0 performance (in other words, not exactly the commented out code below).   \n",
        "\n",
        "history = model.fit(train_padded, y_train, epochs=2,\n",
        "                    validation_data=(dev_padded, y_dev), verbose=1)\n",
        "model.save_weights(root_dir+'LSTM-checkpoints/checkpoint.ckpt')\n",
        "with open(root_dir+'LSTM-checkpoints/history.pickle', 'wb') as handle:\n",
        "    pickle.dump(model.history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1624/1624 [==============================] - 146s 89ms/step - loss: 2.0484 - accuracy: 0.2806 - val_loss: 2.0434 - val_accuracy: 0.2809\n",
            "Epoch 2/2\n",
            "1624/1624 [==============================] - 139s 85ms/step - loss: 2.0439 - accuracy: 0.2809 - val_loss: 2.0438 - val_accuracy: 0.2809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWvhs0NOs8L9",
        "outputId": "f20e67ed-c48a-47e7-b818-0ac8efa28fa1"
      },
      "source": [
        "# Just a simple test: will trainin one more epoch help improve the model? Answer is NO!\n",
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size,embedding_dim),   # Embedding layer\n",
        "    tf.keras.layers.LSTM(embedding_dim),   # LSTM layer\n",
        "    tf.keras.layers.Dense(10,activation = 'softmax')   # Final fully-connected layer\n",
        "])\n",
        "model2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy'])\n",
        "model2.load_weights(root_dir+'LSTM-checkpoints/checkpoint.ckpt')\n",
        "history2 = model2.fit(train_padded, y_train, epochs=1,validation_data=(dev_padded, y_dev), verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1624/1624 [==============================] - 139s 84ms/step - loss: 2.0452 - accuracy: 0.2813 - val_loss: 2.0442 - val_accuracy: 0.2809\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}